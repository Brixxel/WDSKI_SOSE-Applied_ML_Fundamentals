{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch erfolgt die Umsetzung verschiedener Aktivierungsfunktionen und Layer-Architekturen auf eine flexible Weise. Hier ist eine detaillierte Betrachtung dieser Implementierungen sowie ihrer Anwendungen:\n",
    "\n",
    "### Aktivierungsfunktionen in PyTorch\n",
    "\n",
    "Aktivierungsfunktionen sind in PyTorch als Module in `torch.nn` verfügbar, und du kannst sie entweder als Teil eines Layers innerhalb des `forward`-Durchlaufs verwenden oder sie funktionsweise aufrufen.\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**:\n",
    "   - Einfache und weithin verwendete Aktivierungsfunktion, die alle negativen Input-Werte auf 0 setzt.\n",
    "   - **PyTorch-Implementierung**:\n",
    "     ```python\n",
    "     relu = nn.ReLU()\n",
    "     output = relu(input)\n",
    "     ```\n",
    "\n",
    "2. **Sigmoid**:\n",
    "   - Komprimiert die Eingaben auf einen Bereich zwischen 0 und 1.\n",
    "   - **PyTorch-Implementierung**:\n",
    "     ```python\n",
    "     sigmoid = nn.Sigmoid()\n",
    "     output = sigmoid(input)\n",
    "     ```\n",
    "\n",
    "3. **Tanh**:\n",
    "   - Skaliert die Eingaben auf einen Bereich zwischen -1 und 1.\n",
    "   - **PyTorch-Implementierung**:\n",
    "     ```python\n",
    "     tanh = nn.Tanh()\n",
    "     output = tanh(input)\n",
    "     ```\n",
    "\n",
    "4. **Leaky ReLU, Parametric ReLU**:\n",
    "   - Leaky ReLU lässt einen kleinen Teil der negativen Inputs durch, um das Problem des \"tot-Relu\" zu adressieren.\n",
    "   - **PyTorch-Implementierung für Leaky ReLU**:\n",
    "     ```python\n",
    "     leaky_relu = nn.LeakyReLU(negative_slope=0.01)  # negative_slope ist der Faktor für negative Werte\n",
    "     output = leaky_relu(input)\n",
    "     ```\n",
    "\n",
    "   - **Parametric ReLU**, bei der die Neigung der negativen Inputs während des Trainings lernbar ist:\n",
    "     ```python\n",
    "     prelu = nn.PReLU()\n",
    "     output = prelu(input)\n",
    "     ```\n",
    "\n",
    "### Layer-Architekturen in PyTorch\n",
    "\n",
    "1. **Dense (Fully Connected) Layers**:\n",
    "   - Vollständig verbundene Layer, bei denen jedes Neuron mit jedem Neuron der vorherigen Schicht verbunden ist.\n",
    "   - **PyTorch-Implementierung**:\n",
    "     ```python\n",
    "     linear_layer = nn.Linear(in_features=64, out_features=10)\n",
    "     output = linear_layer(input)\n",
    "     ```\n",
    "\n",
    "2. **Convolutional Layers**:\n",
    "   - Arbeiten mit Filtern, um räumlich lokale Muster zu entdecken. Besonders wichtig für die Bildverarbeitung.\n",
    "   - **PyTorch-Implementierung**:\n",
    "     ```python\n",
    "     conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "     output = conv_layer(input)\n",
    "     ```\n",
    "\n",
    "3. **Recurrent Layers**:\n",
    "   - Verwendet in zeitabhängigen und sequenziellen Daten, um historische Informationen zu speichern.\n",
    "   - **PyTorch-Implementierung für eine einfache RNN**:\n",
    "     ```python\n",
    "     rnn_layer = nn.RNN(input_size=10, hidden_size=20, num_layers=2)\n",
    "     output, hidden = rnn_layer(input)\n",
    "     ```\n",
    "\n",
    "   - **LSTM und GRU**, fortgeschrittene RNN-Varianten, die besser helfen, über längere Sequenzen hinweg Informationen zu behalten:\n",
    "     ```python\n",
    "     lstm_layer = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)\n",
    "     gru_layer = nn.GRU(input_size=10, hidden_size=20, num_layers=2)\n",
    "     ```\n",
    "\n",
    "Jede dieser Architekturen und Aktivierungsfunktionen ist optimiert für spezifische Arten von Daten und Anwendungen. Die Wahl hängt von der Art der zu lösenden Aufgabe ab, wie z.B. Bildklassifizierung (Convolutional Netzwerke) oder Sprachverarbeitung (Recurrent Netzwerke). PyTorchs flexible Implementierung ermöglicht es Entwicklern, diese Komponenten zu kombinieren und anzupassen, um ein Modell zu erstellen, das ihren Anforderungen entspricht."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
